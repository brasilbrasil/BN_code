plot(r)
plot(dist / 1000)
r[r==1]=NA
plot(r)
library(raster)
library(stringr)
library(dismo)
root_dir="//10.0.0.5/data2$//PICCC_analysis/community_SRE/"
necessary_data_dir=paste0(root_dir,"necessary_data/")
original_veg_map=raster(paste(necessary_data_dir, "landfire_BPS_reclass_500m.tif", sep=""))
coms_less_than_10sqkm=c(1, 4, 8)
jnk=data.frame(VALUE=coms_less_than_10sqkm, New.Value=rep(0, length(coms_less_than_10sqkm)))
original_veg_map=subs(original_veg_map,jnk,by=1,which=2,subsWithNA=F)
original_BPS=raster(paste(necessary_data_dir, "landfire_BPS_reclass_500m_originalClasses.tif", sep=""))
plot(original_BPS)
original_BPS[original_BPS==12]=NA
plot(original_BPS)
plot(original_veg_map)
original_veg_map[original_veg_map==0]=NA
plot(original_veg_map)
jnk=original_veg_map>0
plot(jnk)
bin_modeled=original_veg_map>0
plot(bin_modeled)
bin_actual=original_BPS>0
plot(bin_actual)
sum(bin_modeled)
cellStats(bin_modeled, sum)
actual_nCells=cellStats(bin_actual, sum)
actual_nCells
library(raster)
library(stringr)
library(dismo)
root_dir="//10.0.0.5/data2$//PICCC_analysis/community_SRE/"
necessary_data_dir=paste0(root_dir,"necessary_data/")
original_veg_map=raster(paste(necessary_data_dir, "landfire_BPS_reclass_500m.tif", sep=""))
coms_less_than_10sqkm=c(1, 4, 8)
jnk=data.frame(VALUE=coms_less_than_10sqkm, New.Value=rep(0, length(coms_less_than_10sqkm)))
original_veg_map=subs(original_veg_map,jnk,by=1,which=2,subsWithNA=F)
original_veg_map[original_veg_map==0]=NA
plot(original_veg_map)
bin_modeled=original_veg_map>0
plot(bin_modeled)
modeled_nCells=cellStats(bin_modeled, sum)
modeled_nCells
original_BPS=raster(paste(necessary_data_dir, "landfire_BPS_reclass_500m_originalClasses.tif", sep=""))
plot(original_BPS)
bin_all=original_BPS>0
actual_nCells=cellStats(bin_all, sum)
plot(bin_all)
actual_nCells=cellStats(bin_all, sum)
actual_nCells
original_BPS[original_BPS==12]=NA
plot(original_BPS)
bin_actual=original_BPS>0
plot(bin_actual)
actual_nCells=cellStats(bin_actual, sum)
actual_nCells
original_BPS=raster(paste(necessary_data_dir, "landfire_BPS_reclass_500m_originalClasses.tif", sep=""))
plot(original_BPS)
bin_all=original_BPS>0
plot(bin_all)
all_nCells=cellStats(bin_all, sum)
original_BPS[original_BPS==12]=NA
plot(original_BPS)
bin_actual=original_BPS>0
plot(bin_actual)
actual_nCells=cellStats(bin_actual, sum)
pct_modeled_veg=modeled_nCells/actual_nCells
pct_modeled_all=modeled_nCells/all_nCells
pct_modeled_veg
pct_modeled_all
original_BPS=raster(paste(necessary_data_dir, "landfire_BPS_reclass_500m_originalClasses.tif", sep=""))
plot(original_BPS)
plot(original_BPS==18)
plot(original_BPS==17)
plot(original_BPS==16)
plot(original_BPS==15)
plot(original_BPS==4)
plot(original_BPS==8)
plot(original_BPS==13)
Iter_vals=c(1:16) the values to iterate the function with
fx_parallel_run=function(Iter_val, multiplier){ #junk function with 2 arguments
jnk=round(runif(1)*multiplier)
jnk1=runif(jnk)
for (i in 1:length(jnk1)){
jnk1[i]=(jnk1[i]*runif(1))+Iter_val
}
return(jnk1)
}
require(snowfall)
cpucores=as.integer(Sys.getenv('NUMBER_OF_PROCESSORS')) #by default snowfall will use the total number of processors, so this is not necessary.
#However, if you are using the machine for other purposes,
#you can adapt this line to leave at least a core or two free
#so the computer is still functional for multi tasking.
sfInit( parallel=T, cpus=cpucores) #
sfExportAll()
results=sfLapply(Iter_vals,fun=fx_parallel_run, multiplier=80) #extra function arguments go after the first two sfLapply arguments
sfRemoveAll()
sfStop()
Iter_vals=as.list(c(1:16))# the values to iterate the function with
fx_parallel_run=function(Iter_val, multiplier){ #junk function with 2 arguments
jnk=round(runif(1)*multiplier)
jnk1=runif(jnk)
for (i in 1:length(jnk1)){
jnk1[i]=(jnk1[i]*runif(1))+Iter_val[[1]]
}
return(jnk1)
}
require(snowfall)
cpucores=as.integer(Sys.getenv('NUMBER_OF_PROCESSORS')) #by default snowfall will use the total number of processors, so this is not necessary.
#However, if you are using the machine for other purposes,
#you can adapt this line to leave at least a core or two free
#so the computer is still functional for multi tasking.
sfInit( parallel=T, cpus=cpucores) #
sfExportAll()
results=sfLapply(Iter_vals,fun=fx_parallel_run, multiplier=800) #extra function arguments go after the first two sfLapply arguments
sfRemoveAll()
sfStop()
results
Iter_vals=as.list(c(1:16))# the values to iterate the function with
fx_parallel_run=function(Iter_val, multiplier){ #junk function with 2 arguments
jnk=round(runif(1)*multiplier)
jnk1=runif(jnk)
for (i in 1:length(jnk1)){
jnk1[i]=(jnk1[i]*runif(1))+Iter_val[[1]]
}
return(jnk1)
}
require(snowfall)
cpucores=as.integer(Sys.getenv('NUMBER_OF_PROCESSORS')) #by default snowfall will use the total number of processors, so this is not necessary.
#However, if you are using the machine for other purposes,
#you can adapt this line to leave at least a core or two free
#so the computer is still functional for multi tasking.
sfInit( parallel=T, cpus=cpucores) #
sfExportAll()
results=sfLapply(Iter_vals,fun=fx_parallel_run, multiplier=800) #extra function arguments go after the first two sfLapply arguments
sfRemoveAll()
sfStop()
results
Iter_vals=as.list(c(1:16))# the values to iterate the function with
fx_parallel_run=function(Iter_val, multiplier){ #junk function with 2 arguments
jnk=round(runif(1)*multiplier)
jnk1=runif(jnk)
for (i in 1:length(jnk1)){
jnk1[i]=(jnk1[i]*runif(1))+Iter_val[[1]]
}
return(jnk1)
}
require(snowfall)
cpucores=as.integer(Sys.getenv('NUMBER_OF_PROCESSORS')) #by default snowfall will use the total number of processors, so this is not necessary.
#However, if you are using the machine for other purposes,
#you can adapt this line to leave at least a core or two free
#so the computer is still functional for multi tasking.
sfInit( parallel=T, cpus=cpucores) #
sfExportAll()
results=sfLapply(Iter_vals,fun=fx_parallel_run, multiplier=800) #extra function arguments go after the first two sfLapply arguments
sfRemoveAll()
sfStop()
?snow::makeSOCKcluster
Take_expected_value <- function(interval_end = 1, points = 100, number_of_trajectories = 1000){
return(
mean(
exp(
replicate(
n = number_of_trajectories,
expr = Max_from_Wiener_on_interval(interval_end, points)
)
)
)
) # This function just replicates max_from_... function, then put values
# to exp function, and calculates mean of all replications.
}
# this function shall not be exported
Max_from_Wiener_on_interval <- function(interval_end = 1, points = 100){
# time increment
Delta <- interval_end/points
# time moments
time <- seq( 0, interval_end, length = points + 1)
# Wiener process
W <-  cumsum( sqrt(Delta) * rnorm( points + 1 ) )
# return max of "Wiener * sqrt(2) - time moment"
return(
max(sqrt(2) * W - time)
)
}
start=system.time()
start=system.time()
start=proc.time()
start
proc.time()-start
start=proc.time()
proc.time()-start
Take_expected_value <- function(interval_end = 1, points = 100, number_of_trajectories = 1000){
return(
mean(
exp(
replicate(
n = number_of_trajectories,
expr = Max_from_Wiener_on_interval(interval_end, points)
)
)
)
) # This function just replicates max_from_... function, then put values
# to exp function, and calculates mean of all replications.
}
# this function shall not be exported
Max_from_Wiener_on_interval <- function(interval_end = 1, points = 100){
# time increment
Delta <- interval_end/points
# time moments
time <- seq( 0, interval_end, length = points + 1)
# Wiener process
W <-  cumsum( sqrt(Delta) * rnorm( points + 1 ) )
# return max of "Wiener * sqrt(2) - time moment"
return(
max(sqrt(2) * W - time)
)
}
start=proc.time()
Take_expected_value()
proc.time()-start
install.packages("snowfall")
require(snowfall)
cpucores=as.integer(Sys.getenv('NUMBER_OF_PROCESSORS'))
sfInit( parallel=T, cpus=cpucores) #
sfExportAll()
start=proc.time()
results=sfLapply(as.list(c(1:8)),fun=Take_expected_value, points =10^6, number_of_trajectories = 10^6)
start=proc.time()
Take_expected_value(interval_end = 1)
proc.time()-start
start=proc.time()
Take_expected_value(interval_end = 2)
proc.time()-start
start=proc.time()
Take_expected_value(interval_end = 3)
proc.time()-start
start=proc.time()
Take_expected_value(interval_end = 100)
proc.time()-start
start=proc.time()
Take_expected_value(interval_end = 100, points =10^6, number_of_trajectories = 10^6)
start=proc.time()
Take_expected_value(interval_end = 100)
proc.time()-start
start=proc.time()
results=sfLapply(as.list(c(1:8)),fun=Take_expected_value, points = 100, number_of_trajectories = 1000)
? sfLapply
Take_expected_value <- function(interval_end = 1, points = 100, number_of_trajectories = 1000){
return(
mean(
exp(
replicate(
n = number_of_trajectories,
expr = Max_from_Wiener_on_interval(interval_end, points)
)
)
)
) # This function just replicates max_from_... function, then put values
# to exp function, and calculates mean of all replications.
}
# this function shall not be exported
Max_from_Wiener_on_interval <- function(interval_end = 1, points = 100){
# time increment
Delta <- interval_end/points
# time moments
time <- seq( 0, interval_end, length = points + 1)
# Wiener process
W <-  cumsum( sqrt(Delta) * rnorm( points + 1 ) )
# return max of "Wiener * sqrt(2) - time moment"
return(
max(sqrt(2) * W - time)
)
}
#test first run in serial
start=proc.time()
Take_expected_value(interval_end = 100)
proc.time()-start
sfStop()
sfStop()
sfStop()
Take_expected_value <- function(interval_end = 1, points = 100, number_of_trajectories = 1000){
return(
mean(
exp(
replicate(
n = number_of_trajectories,
expr = Max_from_Wiener_on_interval(interval_end, points)
)
)
)
) # This function just replicates max_from_... function, then put values
# to exp function, and calculates mean of all replications.
}
# this function shall not be exported
Max_from_Wiener_on_interval <- function(interval_end = 1, points = 100){
# time increment
Delta <- interval_end/points
# time moments
time <- seq( 0, interval_end, length = points + 1)
# Wiener process
W <-  cumsum( sqrt(Delta) * rnorm( points + 1 ) )
# return max of "Wiener * sqrt(2) - time moment"
return(
max(sqrt(2) * W - time)
)
}
#test first run in serial
start=proc.time()
Take_expected_value(interval_end = 100)
proc.time()-start
start=proc.time()
Take_expected_value(interval_end = 100, points =10^6, number_of_trajectories = 10^6)
Take_expected_value <- function(interval_end = 1, points = 100, number_of_trajectories = 1000){
return(
mean(
exp(
replicate(
n = number_of_trajectories,
expr = Max_from_Wiener_on_interval(interval_end, points)
)
)
)
) # This function just replicates max_from_... function, then put values
# to exp function, and calculates mean of all replications.
}
# this function shall not be exported
Max_from_Wiener_on_interval <- function(interval_end = 1, points = 100){
# time increment
Delta <- interval_end/points
# time moments
time <- seq( 0, interval_end, length = points + 1)
# Wiener process
W <-  cumsum( sqrt(Delta) * rnorm( points + 1 ) )
# return max of "Wiener * sqrt(2) - time moment"
return(
max(sqrt(2) * W - time)
)
}
install.packages("snowfall")
require(snowfall)
cpucores=as.integer(Sys.getenv('NUMBER_OF_PROCESSORS'))
sfInit( parallel=T, cpus=cpucores) #
sfExportAll()
start=proc.time()
results=sfLapply(as.list(c(1:8)),fun=Take_expected_value)
proc.time()-start
sfRemoveAll()
sfStop()
start=proc.time()
results=sfLapply(as.list(c(1:8)),fun=Take_expected_value)
proc.time()-start
results
Take_expected_value <- function(interval_end = 1, points = 100, number_of_trajectories = 1000){
return(
mean(
exp(
replicate(
n = number_of_trajectories,
expr = Max_from_Wiener_on_interval(interval_end, points)
)
)
)
) # This function just replicates max_from_... function, then put values
# to exp function, and calculates mean of all replications.
}
# this function shall not be exported
Max_from_Wiener_on_interval <- function(interval_end = 1, points = 100){
# time increment
Delta <- interval_end/points
# time moments
time <- seq( 0, interval_end, length = points + 1)
# Wiener process
W <-  cumsum( sqrt(Delta) * rnorm( points + 1 ) )
# return max of "Wiener * sqrt(2) - time moment"
return(
max(sqrt(2) * W - time)
)
}
#test first run in serial
# start=proc.time()
# Take_expected_value(interval_end = 100)
# proc.time()-start
#
# start=proc.time()
# Take_expected_value(interval_end = 100, points =10^6, number_of_trajectories = 10^6)
# proc.time()-start
#parallel run
install.packages("snowfall")
require(snowfall)
cpucores=as.integer(Sys.getenv('NUMBER_OF_PROCESSORS'))
sfInit( parallel=T, cpus=cpucores) #
sfExportAll()
start=proc.time()
results=sfLapply(as.list(c(1:1000)),fun=Take_expected_value)
proc.time()-start
sfRemoveAll()
sfStop()
results
install.packages("snowfall")
require(snowfall)
cpucores=as.integer(Sys.getenv('NUMBER_OF_PROCESSORS'))
sfInit( parallel=F, cpus=cpucores) #
sfExportAll()
start=proc.time()
results=sfLapply(as.list(c(1:1000)),fun=Take_expected_value)
require(snowfall)
cpucores=as.integer(Sys.getenv('NUMBER_OF_PROCESSORS'))
sfInit( parallel=F, cpus=cpucores) #
sfExportAll()
start=proc.time()
results=sfLapply(as.list(c(1:1000)),fun=Take_expected_value)
proc.time()-start
sfRemoveAll()
sfStop()
? sleep
Sys.sleep(3)
require(snowfall)
sfInit( parallel=T, type='SOCK', socketHosts=c("10.0.0.110", "10.0.0.109") #
)
library(snow)
? makeSOCKcluster
cl <- makeSOCKcluster(c("10.0.0.110", "10.0.0.109"), manual=TRUE, outfile="")
cl <- makeSOCKcluster(c("10.0.0.110", "10.0.0.109"), manual=TRUE, outfile="", port=3389)
install.packages("installr")
updateR()
library(installr)
updateR()
library(raster)
.libPaths()
sigma <- c(1,2,3)
mu <- c(4,5,6)
rp <- vector('expression',3)
i=1
for (i in 1:length(sigma)){
jnk=paste("paste(sigma == ", sigma[i],",', ',", "mu == ", mu[i],")")
rp[i]=parse(text=jnk)
}
plot(0);legend(x="bottomleft",legend=rp)
dir="D:/dropbox/code/community_SRE_code/"
files=list.files(path = dir, pattern = '..r$|..R$', all.files = FALSE,
full.names = T, recursive = T,
ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)
file=files[1]
text <- readLines(file,encoding="UTF-8")
text=readChar(file, file.info(file)$size)
patt="^library\\(.*\\)$"
a=grep(patt, text)
a
library()
file
files
file=files[79]
file
text <- readLines(file,encoding="UTF-8")
text=readChar(file, file.info(file)$size)
patt="^library\\(.*\\)$"
a=grep(patt, text)
a
text
sessionInfo()
packinfo <- installed.packages (fields = c ("Package", "Version"))
packinfo
sessionInfo() #for ones loads
glob2rx("library*a")
patt="^library\\(.*\\)$"
grep(patt, text)
? grep
as.character(text)
test="I will install library(test) now"
a=grep(patt, test)
a
test="I will install librarytesta now"
a=grep(patt, test)
a
patt="^library.*a$"
test="I will install librarytesta now"
a=grep(patt, test)
a
glob2rx("library*")
a=grep(glob2rx("library*"), test)
a
test
test="I will install library testa now"
a=grep(glob2rx("library*"), test)
a
grep(glob2rx("library*"), test, value=T)
t1=("IGF2, IGF2AS, INS, TH")
t2=c("TH")
t3=c("THZH")
t4=c("ZGTH")
grep("\\bTH\\b",t3, value=T)
grep ("\\<TH\\>", t1)
grep ("\\<TH\\>", t1, value=T)
library("gdata") #for xlsx input
rm(list = ls()) #remove all past worksheet variables
setwd("C:/Users/lfortini/Dropbox/code/BN_code/other_analyses/CPT creation/")
#do not place conditional nodes up front of array, but as first parent in CPT in genie
####read xlsx file
data.xlsx <- read.xls("P_BN_model3_expert2.xlsx")
node.r <- grep("#",data.xlsx[,1])  #row numbers for new Node
node.n <- length(node.r)  #number of Nodes
node.c <- c(3:11)  #column numbers for nodes
node.cond <- data.xlsx[,"Conditional"]
node=6
library("gdata") #for xlsx input
setwd("C:/Users/lfortini/Dropbox/code/BN_code/other_analyses/cpt_creation/")
setwd("D:/Dropbox/code/BN_code/other_analyses/cpt_creation/")
data.xlsx <- read.xls("P_BN_model3_expert2.xlsx")
data.xlsx <- read.xls("P_BN_model3_expert2.xlsx")
data.xlsx <- read.xls("P_BN_model3_expert2.xlsx")
